{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - RAW to CLEAN (DuckDB) Â· OpenBDAP Saldi storici\n",
        "\n",
        "Questo notebook:\n",
        "- legge il CSV RAW salvato in `data/raw/<PROJECT>/<RUN_ID>/..._raw.csv`\n",
        "- crea una tabella `clean` con rename a `snake_case` e cast coerenti\n",
        "- applica una policy esplicita per null e parsing numerico\n",
        "- esegue validazioni minime sul dataset CLEAN\n",
        "- esporta parquet e metadata in `data/clean/<PROJECT>/<RUN_ID>/`\n",
        "\n",
        "Policy di parsing:\n",
        "- `\"\"`, `\" \"`, `\"n.d.\"`, `\"nd\"`, `\"N.D.\"`, `\"null\"` -> `NULL`\n",
        "- `-` e mantenuto come segno meno valido, non come null\n",
        "- `.` e trattato come separatore decimale canonico\n",
        "- `,` viene convertito a `.` solo se presente\n",
        "- valori con `%` vengono convertiti dividendo per 100\n",
        "- nessun ricalcolo o modifica semantica dei valori economici\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import json\n",
        "import hashlib\n",
        "import os\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "import duckdb\n",
        "\n",
        "PROJECT = \"openbdap_rendiconto_saldi_storico\"\n",
        "DATASET_SLUG = \"rendiconto_pubblicato_serie_storica_saldi\"\n",
        "RAW_RUN_ID = None\n",
        "CLEAN_RUN_ID = None\n",
        "DELIM = \";\"\n",
        "ENCODING = \"utf-8\"\n",
        "MIN_WARN_ROWS = 10\n",
        "\n",
        "SPECIAL_NULLS = {\"\", \" \", \"n.d.\", \"nd\", \"N.D.\", \"null\", \"NULL\"}\n",
        "\n",
        "SEMANTIC_MAP = {\n",
        "    \"ANNO\": \"esercizio_finanziario\",\n",
        "    \"RISPARMIO_PUBBLICO\": \"risparmio_pubblico\",\n",
        "    \"SALDO_NETTO\": \"saldo_netto_da_finanziare\",\n",
        "    \"INDEBITAMENTO_NETTO\": \"indebitamento_netto\",\n",
        "    \"RICORSO_MERCATO\": \"ricorso_al_mercato\",\n",
        "    \"AVANZO_PRIMARIO\": \"avanzo_primario\",\n",
        "    \"SPESE_CORRENTI\": \"spese_correnti\",\n",
        "    \"SPESE_INTERESSI\": \"spese_per_interessi\",\n",
        "    \"SPESE_CONTO_CAPITALE\": \"spese_in_conto_capitale\",\n",
        "    \"SPESE_ACQ_ATT_FINE\": \"spese_acquisizione_attivita_finanziarie\",\n",
        "    \"SPESE_RIMBORSO_PRESTITI\": \"spese_per_rimborso_prestiti\",\n",
        "    \"SPESE_COMPLESSIVE\": \"spese_complessive\",\n",
        "    \"SPESE_FINALI\": \"spese_finali\",\n",
        "    \"SPESE_FIN_NETTO_ATT_FIN\": \"spese_finali_netto_att_fin\",\n",
        "    \"ENTRATE_TRIBUTARIE\": \"entrate_tributarie\",\n",
        "    \"ENTRATE_EXTRA_TRIBUTARIE\": \"entrate_extra_tributarie\",\n",
        "    \"ENTR_ALIEN_PATR_RISCOS\": \"entrate_alienazioni_patrimoniali_e_riscossioni\",\n",
        "    \"RISCOSSIONE_CREDITI\": \"riscossione_crediti\",\n",
        "    \"ENTR_ACCENSIONE_PRESTITI\": \"entrate_accensione_prestiti\",\n",
        "    \"ENTRATE_FINALI\": \"entrate_finali\",\n",
        "    \"ENTR_FIN_NETTO_RISCO_CRED\": \"entrate_fin_netto_riscossione_crediti\",\n",
        "    \"ENTRATE_CORRENTI\": \"entrate_correnti\",\n",
        "}\n",
        "\n",
        "DESCRIPTIONS = {\n",
        "    \"esercizio_finanziario\": \"Anno di esercizio finanziario.\",\n",
        "    \"risparmio_pubblico\": \"Saldo di risparmio pubblico.\",\n",
        "    \"saldo_netto_da_finanziare\": \"Saldo netto da finanziare.\",\n",
        "    \"indebitamento_netto\": \"Indebitamento netto.\",\n",
        "    \"ricorso_al_mercato\": \"Ricorso al mercato.\",\n",
        "    \"avanzo_primario\": \"Avanzo primario.\",\n",
        "}\n",
        "\n",
        "def maybe_mount_drive() -> None:\n",
        "    if \"google.colab\" not in str(get_ipython().__class__):\n",
        "        return\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "def discover_root() -> Path:\n",
        "    env_root = os.getenv(\"DCL_ROOT\")\n",
        "    if env_root:\n",
        "        return Path(env_root)\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for candidate in [cwd, *cwd.parents]:\n",
        "        if (candidate / 'data').exists() and (candidate / 'notebooks').exists():\n",
        "            return candidate\n",
        "    if Path('/content/drive/MyDrive/DataCivicLab').exists():\n",
        "        return Path('/content/drive/MyDrive/DataCivicLab')\n",
        "    return cwd\n",
        "\n",
        "def latest_run_dir(root: Path) -> Path:\n",
        "    run_dirs = sorted([p for p in root.iterdir() if p.is_dir()], key=lambda p: p.name)\n",
        "    if not run_dirs:\n",
        "        raise FileNotFoundError(f'No run dirs in: {root}')\n",
        "    return run_dirs[-1]\n",
        "\n",
        "def sha256_file(path: Path) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def to_snake(value: str) -> str:\n",
        "    value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = value.strip().lower()\n",
        "    value = re.sub(r'[^\\w]+', '_', value)\n",
        "    value = re.sub(r'_+', '_', value).strip('_')\n",
        "    if not value:\n",
        "        value = 'col'\n",
        "    if value[0].isdigit():\n",
        "        value = 'c_' + value\n",
        "    return value\n",
        "\n",
        "maybe_mount_drive()\n",
        "\n",
        "ROOT = discover_root()\n",
        "RAW_ROOT = ROOT / 'data' / 'raw' / PROJECT\n",
        "CLEAN_ROOT = ROOT / 'data' / 'clean' / PROJECT\n",
        "\n",
        "raw_run_dir = latest_run_dir(RAW_ROOT) if RAW_RUN_ID is None else (RAW_ROOT / RAW_RUN_ID)\n",
        "RAW_RUN_ID = raw_run_dir.name\n",
        "raw_csv = raw_run_dir / f'{DATASET_SLUG}_raw.csv'\n",
        "if not raw_csv.exists():\n",
        "    cands = sorted(raw_run_dir.glob('*_raw.csv'))\n",
        "    if not cands:\n",
        "        raise FileNotFoundError(f'No *_raw.csv in {raw_run_dir}')\n",
        "    raw_csv = cands[0]\n",
        "\n",
        "CLEAN_RUN_ID = RAW_RUN_ID if CLEAN_RUN_ID is None else CLEAN_RUN_ID\n",
        "CLEAN_DIR = CLEAN_ROOT / CLEAN_RUN_ID\n",
        "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_PARQUET = CLEAN_DIR / 'saldi_storico.parquet'\n",
        "OUT_MAPPING = CLEAN_DIR / 'columns_mapping_raw_to_clean.json'\n",
        "OUT_PROFILE = CLEAN_DIR / 'profile_clean.json'\n",
        "OUT_VALIDATE = CLEAN_DIR / 'validate_clean.json'\n",
        "OUT_DICT = CLEAN_DIR / 'data_dictionary.json'\n",
        "OUT_MANIFEST = CLEAN_DIR / 'clean_manifest.json'\n",
        "\n",
        "print('ROOT:', ROOT)\n",
        "print('RAW_RUN_ID:', RAW_RUN_ID)\n",
        "print('raw_csv:', raw_csv)\n",
        "print('CLEAN_DIR:', CLEAN_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- DUCKDB: load raw + parse policy ---\n",
        "con = duckdb.connect()\n",
        "\n",
        "con.execute(f\"\"\"\n",
        "CREATE OR REPLACE TABLE raw AS\n",
        "SELECT * FROM read_csv(\n",
        "  '{raw_csv.as_posix()}',\n",
        "  delim='{DELIM}',\n",
        "  header=true,\n",
        "  all_varchar=true,\n",
        "  encoding='{ENCODING}'\n",
        ");\n",
        "\"\"\")\n",
        "\n",
        "null_tokens = ', '.join([\"'\" + token.replace(\"'\", \"''\") + \"'\" for token in sorted(SPECIAL_NULLS)])\n",
        "\n",
        "con.execute(f\"\"\"\n",
        "CREATE OR REPLACE MACRO clean_token(x) AS (\n",
        "  CASE\n",
        "    WHEN x IS NULL THEN NULL\n",
        "    WHEN TRIM(CAST(x AS VARCHAR)) IN ({null_tokens}) THEN NULL\n",
        "    ELSE TRIM(CAST(x AS VARCHAR))\n",
        "  END\n",
        ");\n",
        "\"\"\")\n",
        "\n",
        "con.execute(r\"\"\"\n",
        "CREATE OR REPLACE MACRO parse_num(x) AS (\n",
        "  TRY_CAST(\n",
        "    CASE\n",
        "      WHEN clean_token(x) IS NULL THEN NULL\n",
        "      WHEN RIGHT(clean_token(x), 1) = '%' THEN REPLACE(LEFT(clean_token(x), LENGTH(clean_token(x)) - 1), ',', '.')\n",
        "      ELSE REPLACE(clean_token(x), ',', '.')\n",
        "    END\n",
        "  AS DOUBLE\n",
        "  ) / CASE WHEN clean_token(x) IS NOT NULL AND RIGHT(clean_token(x), 1) = '%' THEN 100 ELSE 1 END\n",
        ");\n",
        "\"\"\")\n",
        "\n",
        "raw_cols = [r[1] for r in con.execute(\"PRAGMA table_info('raw')\").fetchall()]\n",
        "YEAR_COL = next((c for c in raw_cols if c.strip().upper() in {'ANNO', 'ESERCIZIO', 'ESERCIZIO_FINANZIARIO'}), None)\n",
        "NUMERIC_COLS = set(SEMANTIC_MAP.keys()) - {'ANNO'}\n",
        "\n",
        "used = set()\n",
        "final_map = {}\n",
        "select_exprs = []\n",
        "\n",
        "for c in raw_cols:\n",
        "    new = SEMANTIC_MAP.get(c, to_snake(c))\n",
        "    if new in used:\n",
        "        i = 2\n",
        "        while f'{new}_{i}' in used:\n",
        "            i += 1\n",
        "        new = f'{new}_{i}'\n",
        "    used.add(new)\n",
        "    final_map[c] = new\n",
        "\n",
        "    if YEAR_COL and c == YEAR_COL:\n",
        "        expr = f'TRY_CAST(clean_token(\"{c}\") AS INTEGER) AS \"{new}\"'\n",
        "    elif c in NUMERIC_COLS:\n",
        "        expr = f'parse_num(\"{c}\") AS \"{new}\"'\n",
        "    else:\n",
        "        expr = f'clean_token(\"{c}\") AS \"{new}\"'\n",
        "    select_exprs.append(expr)\n",
        "\n",
        "clean_sql = 'CREATE OR REPLACE TABLE clean AS\\nSELECT\\n  ' + ',\\n  '.join(select_exprs) + '\\nFROM raw;'\n",
        "con.execute(clean_sql)\n",
        "con.execute(f\"COPY clean TO '{OUT_PARQUET.as_posix()}' (FORMAT PARQUET);\")\n",
        "con.execute(\"SELECT * FROM clean ORDER BY 1 LIMIT 5\").df()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- validations + profile + metadata ---\n",
        "n_rows = con.execute(\"SELECT COUNT(*) FROM clean\").fetchone()[0]\n",
        "schema_rows = con.execute(\"PRAGMA table_info('clean')\").fetchall()\n",
        "cols = [r[1] for r in schema_rows]\n",
        "types = {r[1]: r[2] for r in schema_rows}\n",
        "\n",
        "null_exprs = ', '.join([f'SUM(CASE WHEN \"{c}\" IS NULL THEN 1 ELSE 0 END) AS \"{c}\"' for c in cols])\n",
        "nulls_row = con.execute(f'SELECT {null_exprs} FROM clean').fetchone()\n",
        "nulls = dict(zip(cols, map(int, nulls_row)))\n",
        "\n",
        "required_cols = ['esercizio_finanziario']\n",
        "for candidate in ['saldo_netto_da_finanziare', 'indebitamento_netto', 'avanzo_primario']:\n",
        "    if candidate in cols:\n",
        "        required_cols.append(candidate)\n",
        "\n",
        "missing_required = [c for c in required_cols if c not in cols]\n",
        "duplicate_key_rows = None\n",
        "year_min = None\n",
        "year_max = None\n",
        "if 'esercizio_finanziario' in cols:\n",
        "    duplicate_key_rows = con.execute(\"\"\"\n",
        "        SELECT COALESCE(SUM(cnt - 1), 0)\n",
        "        FROM (\n",
        "          SELECT esercizio_finanziario, COUNT(*) AS cnt\n",
        "          FROM clean\n",
        "          GROUP BY 1\n",
        "          HAVING COUNT(*) > 1\n",
        "        ) t\n",
        "    \"\"\").fetchone()[0]\n",
        "    year_min, year_max = con.execute(\n",
        "        'SELECT MIN(esercizio_finanziario), MAX(esercizio_finanziario) FROM clean WHERE esercizio_finanziario IS NOT NULL'\n",
        "    ).fetchone()\n",
        "\n",
        "errors = []\n",
        "warnings = []\n",
        "checks = []\n",
        "\n",
        "checks.append({'name': 'row_count_ge_1', 'ok': n_rows >= 1, 'value': int(n_rows)})\n",
        "if n_rows < 1:\n",
        "    errors.append('Dataset clean vuoto.')\n",
        "if n_rows < MIN_WARN_ROWS:\n",
        "    warnings.append(f'Row count basso: {n_rows} righe, attese preferibilmente >= {MIN_WARN_ROWS}.')\n",
        "\n",
        "checks.append({'name': 'required_columns_present', 'ok': len(missing_required) == 0, 'missing': missing_required})\n",
        "if missing_required:\n",
        "    errors.append('Colonne obbligatorie mancanti: ' + ', '.join(missing_required))\n",
        "\n",
        "if duplicate_key_rows is not None:\n",
        "    checks.append({'name': 'unique_esercizio_finanziario', 'ok': duplicate_key_rows == 0, 'duplicate_key_rows': int(duplicate_key_rows)})\n",
        "    if duplicate_key_rows != 0:\n",
        "        errors.append(f'Duplicati sulla chiave esercizio_finanziario: {duplicate_key_rows}.')\n",
        "    checks.append({'name': 'year_bounds_present', 'ok': year_min is not None and year_max is not None, 'year_min': year_min, 'year_max': year_max})\n",
        "else:\n",
        "    checks.append({'name': 'unique_esercizio_finanziario', 'ok': False, 'duplicate_key_rows': None})\n",
        "    warnings.append('Colonna esercizio_finanziario assente: impossibile validare unicita e range anni.')\n",
        "\n",
        "validate_clean = {\n",
        "    'ok': len(errors) == 0,\n",
        "    'checks': checks,\n",
        "    'errors': errors,\n",
        "    'warnings': warnings,\n",
        "}\n",
        "\n",
        "sample_rows = con.execute('SELECT * FROM clean LIMIT 10').fetchall()\n",
        "sample_rows = [dict(zip(cols, row)) for row in sample_rows]\n",
        "\n",
        "profile_clean = {\n",
        "    'project': PROJECT,\n",
        "    'raw_run_id': RAW_RUN_ID,\n",
        "    'clean_run_id': CLEAN_RUN_ID,\n",
        "    'raw_csv': str(raw_csv),\n",
        "    'clean_parquet': str(OUT_PARQUET),\n",
        "    'n_rows': int(n_rows),\n",
        "    'n_cols': int(len(cols)),\n",
        "    'columns': cols,\n",
        "    'types': types,\n",
        "    'nulls': nulls,\n",
        "    'sample_rows': sample_rows,\n",
        "}\n",
        "\n",
        "data_dictionary = []\n",
        "for col in cols:\n",
        "    sample_value = next((row.get(col) for row in sample_rows if row.get(col) is not None), None)\n",
        "    data_dictionary.append({\n",
        "        'column': col,\n",
        "        'type': types.get(col),\n",
        "        'description': DESCRIPTIONS.get(col, 'Campo derivato dal RAW senza reinterpretazione.'),\n",
        "        'example_value': sample_value,\n",
        "        'parsing_policy': 'trim + null policy + numeric cast dove applicabile',\n",
        "    })\n",
        "\n",
        "OUT_MAPPING.write_text(json.dumps(final_map, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "OUT_PROFILE.write_text(json.dumps(profile_clean, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "OUT_VALIDATE.write_text(json.dumps(validate_clean, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "OUT_DICT.write_text(json.dumps(data_dictionary, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "manifest = {\n",
        "    'project': PROJECT,\n",
        "    'raw_run_id': RAW_RUN_ID,\n",
        "    'clean_run_id': CLEAN_RUN_ID,\n",
        "    'created_utc': datetime.now(timezone.utc).isoformat(),\n",
        "    'inputs': {\n",
        "        'raw_csv': {'path': str(raw_csv), 'sha256': sha256_file(raw_csv)}\n",
        "    },\n",
        "    'outputs': {\n",
        "        'clean_parquet': {'path': str(OUT_PARQUET), 'sha256': sha256_file(OUT_PARQUET)},\n",
        "        'columns_mapping_raw_to_clean': {'path': str(OUT_MAPPING), 'sha256': sha256_file(OUT_MAPPING)},\n",
        "        'profile_clean': {'path': str(OUT_PROFILE), 'sha256': sha256_file(OUT_PROFILE)},\n",
        "        'validate_clean': {'path': str(OUT_VALIDATE), 'sha256': sha256_file(OUT_VALIDATE)},\n",
        "        'data_dictionary': {'path': str(OUT_DICT), 'sha256': sha256_file(OUT_DICT)}\n",
        "    },\n",
        "    'config': {\n",
        "        'delimiter': DELIM,\n",
        "        'encoding': ENCODING,\n",
        "        'null_policy': sorted(SPECIAL_NULLS),\n",
        "        'percent_policy': 'divide_by_100',\n",
        "        'decimal_policy': 'keep_dot_decimal_convert_comma_to_dot'\n",
        "    }\n",
        "}\n",
        "OUT_MANIFEST.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "validate_clean\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output generati nel run CLEAN:\n",
        "- `data/clean/<PROJECT>/<RUN_ID>/saldi_storico.parquet`\n",
        "- `columns_mapping_raw_to_clean.json`\n",
        "- `profile_clean.json`\n",
        "- `validate_clean.json`\n",
        "- `data_dictionary.json`\n",
        "- `clean_manifest.json`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
